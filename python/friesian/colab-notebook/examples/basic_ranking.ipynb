{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02faf67b",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/intel-analytics/BigDL/blob/main/python/friesian/colab-notebook/examples/basic_ranking.ipynb) &nbsp;![](../../../image/GitHub-Mark-32px.png)[View source on GitHub](https://github.com/intel-analytics/BigDL/blob/main/python/friesian/colab-notebook/examples/basic_ranking.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright 2016 The BigDL Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "#\n",
    "# Copyright 2020 The TensorFlow Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "# This example is based on Tensorflow Recommenders example [basic ranking](https://www.tensorflow.org/recommenders/examples/basic_ranking).\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da28ee0c",
   "metadata": {},
   "source": [
    "# Basic Ranking Example\n",
    "\n",
    "In this tutorial, we're going to:\n",
    "\n",
    "1. Use Friesian FeatureTable to get and preprocess the movielens data and split it into a training and test set.\n",
    "2. Convert the preprocessed FeatureTable to an Orca TF Dataset and do some further data preprocessing.\n",
    "3. Fit and evaluate the TFRS ranking model using Orca TF Estimator and Orca TF Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c388dc8",
   "metadata": {},
   "source": [
    "# Environment Preparation\n",
    "\n",
    "### Install Java 8\n",
    "\n",
    "Run the cell on the **Google Colab** to install jdk 1.8.\n",
    "\n",
    "**Note**: if you run this notebook on your computer, root permission is required when running the cell to install Java 8. (You may ignore this cell if Java 8 has already been set up in your computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18f6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install jdk8\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "import os\n",
    "# Set environment variable JAVA_HOME.\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f6be1",
   "metadata": {},
   "source": [
    "### Install BigDL Friesian\n",
    "\n",
    "You can install the latest pre-release version using pip install --pre --upgrade bigdl-friesian[train]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e513468d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bigdl-friesian[train] in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (2.1.0b20220613)\n",
      "Requirement already satisfied: bigdl-orca==2.1.0b20220613 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-friesian[train]) (2.1.0b20220613)\n",
      "Requirement already satisfied: bigdl-tf==0.14.0.dev1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.14.0.dev1)\n",
      "Requirement already satisfied: pyzmq in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (22.3.0)\n",
      "Requirement already satisfied: packaging in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (21.3)\n",
      "Requirement already satisfied: bigdl-math==0.14.0.dev1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.14.0.dev1)\n",
      "Requirement already satisfied: filelock in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (3.7.1)\n",
      "Requirement already satisfied: bigdl-dllib==2.1.0b20220613 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (2.1.0b20220613)\n",
      "Requirement already satisfied: conda-pack==0.3.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-dllib==2.1.0b20220613->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.3.1)\n",
      "Requirement already satisfied: pyspark==2.4.6 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-dllib==2.1.0b20220613->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (2.4.6)\n",
      "Requirement already satisfied: bigdl-core==2.1.0b20220321 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-dllib==2.1.0b20220613->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (2.1.0b20220321)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-dllib==2.1.0b20220613->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.21.4)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-dllib==2.1.0b20220613->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from conda-pack==0.3.1->bigdl-dllib==2.1.0b20220613->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (58.0.4)\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from pyspark==2.4.6->bigdl-dllib==2.1.0b20220613->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.10.7)\n",
      "Requirement already satisfied: aiohttp==3.8.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (3.8.1)\n",
      "Requirement already satisfied: aioredis==1.3.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.3.1)\n",
      "Requirement already satisfied: psutil in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (5.9.1)\n",
      "Requirement already satisfied: async-timeout==4.0.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (4.0.1)\n",
      "Requirement already satisfied: hiredis==2.0.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (2.0.0)\n",
      "Requirement already satisfied: setproctitle in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.2.3)\n",
      "Requirement already satisfied: ray[default]==1.9.2 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from aiohttp==3.8.1->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from aiohttp==3.8.1->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from aiohttp==3.8.1->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.3.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from aiohttp==3.8.1->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from aiohttp==3.8.1->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (2.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from aiohttp==3.8.1->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (4.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from aiohttp==3.8.1->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from aiohttp==3.8.1->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.7.2)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (3.19.4)\n",
      "Requirement already satisfied: jsonschema in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (4.4.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (8.1.3)\n",
      "Requirement already satisfied: redis>=3.5.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (4.1.4)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.0.4)\n",
      "Requirement already satisfied: pyyaml in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (6.0)\n",
      "Requirement already satisfied: grpcio>=1.28.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.44.0)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.4.0.dev2)\n",
      "Requirement already satisfied: colorful in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.6.0a1)\n",
      "Requirement already satisfied: smart-open in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (6.0.0)\n",
      "Requirement already satisfied: gpustat>=1.0.0b1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.0.0b1)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.13.1)\n",
      "Requirement already satisfied: requests in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (2.28.0)\n",
      "Requirement already satisfied: aiohttp-cors in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.7.0)\n",
      "Requirement already satisfied: opencensus in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.9.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from click>=7.0->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (4.11.2)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from gpustat>=1.0.0b1->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (7.352.0)\n",
      "Requirement already satisfied: blessed>=1.17.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from gpustat>=1.0.0b1->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.19.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wcwidth>=0.1.4 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from blessed>=1.17.1->gpustat>=1.0.0b1->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.2.5)\n",
      "Requirement already satisfied: deprecated>=1.2.3 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from redis>=3.5.0->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.2.13)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from deprecated>=1.2.3->redis>=3.5.0->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.14.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from importlib-metadata->click>=7.0->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from packaging->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (3.0.7)\n",
      "Requirement already satisfied: idna>=2.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp==3.8.1->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (3.3)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from jsonschema->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from jsonschema->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.18.1)\n",
      "Requirement already satisfied: opencensus-context>=0.1.2 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from opencensus->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.2.dev0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from opencensus->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (2.8.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.56.2)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (2.7.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (5.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from requests->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from requests->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.26.9)\n"
     ]
    }
   ],
   "source": [
    "# Install latest pre-release version of BigDL Friesian \n",
    "# Installing BigDL Friesian from pip will automatically install pyspark and their dependencies.\n",
    "!pip install --pre --upgrade bigdl-friesian[train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e561e354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (2.9.1)\n",
      "Requirement already satisfied: tensorflow-recommenders in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: packaging in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (1.21.4)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (1.44.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from packaging->tensorflow) (3.0.7)\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "!pip install tensorflow tensorflow-recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0a2947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 11:44:11.205679: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-14 11:44:11.205709: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fdddd1",
   "metadata": {},
   "source": [
    "## Distributed TFRS using Orca and Friesian APIs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1c7a3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, please use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "  warnings.warn(warning_msg)\n",
      "/home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/zoo_engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, you are recommended to use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from bigdl.friesian.models import TFRSModel\n",
    "from bigdl.orca import init_orca_context, stop_orca_context\n",
    "from bigdl.orca import OrcaContext\n",
    "from bigdl.friesian.feature import FeatureTable\n",
    "from bigdl.orca.learn.tf2 import Estimator\n",
    "from bigdl.orca.data.tf.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c12eaa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing orca context\n",
      "Current pyspark location is : /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py\n",
      "Start to getOrCreate SparkContext\n",
      "pyspark_submit_args is:  --driver-class-path /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/share/orca/lib/bigdl-orca-spark_2.4.6-2.1.0-SNAPSHOT-jar-with-dependencies.jar:/home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/share/dllib/lib/bigdl-dllib-spark_2.4.6-2.1.0-SNAPSHOT-jar-with-dependencies.jar:/home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/share/friesian/lib/bigdl-friesian-spark_2.4.6-2.1.0-SNAPSHOT-jar-with-dependencies.jar:/home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/share/core/lib/all-2.1.0-20220314.094552-2.jar pyspark-shell \n",
      "[main] WARN  org.apache.spark.util.Utils  - Your hostname, yina-intel resolves to a loopback address: 127.0.1.1; using 10.239.158.177 instead (on interface enp0s31f6)\n",
      "[main] WARN  org.apache.spark.util.Utils  - Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-14 11:44:20,094 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "2022-06-14 11:44:20,096 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "2022-06-14 11:44:20,097 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "2022-06-14 11:44:20,097 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "22-06-14 11:44:20 [Thread-4] INFO  Engine$:121 - Auto detect executor number and executor cores number\n",
      "22-06-14 11:44:20 [Thread-4] INFO  Engine$:123 - Executor number is 1 and executor cores number is 1\n",
      "22-06-14 11:44:20 [Thread-4] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 27\n",
      "[Thread-4] WARN  org.apache.spark.SparkContext  - Using an existing SparkContext; some configuration may not take effect.\n",
      "22-06-14 11:44:20 [Thread-4] INFO  Engine$:456 - Find existing spark context. Checking the spark conf...\n",
      "cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.Sample\n",
      "BigDLBasePickler registering: bigdl.dllib.utils.common  Sample\n",
      "cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.EvaluatedResult\n",
      "BigDLBasePickler registering: bigdl.dllib.utils.common  EvaluatedResult\n",
      "cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.JTensor\n",
      "BigDLBasePickler registering: bigdl.dllib.utils.common  JTensor\n",
      "cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.JActivitySuccessfully got a SparkContext\n",
      "BigDLBasePickler registering: bigdl.dllib.utils.common  JActivity\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,compact,1,0\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_SETTINGS=1\n",
      "   OMP_NUM_THREADS=1\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=144\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_HAND_THREAD=false\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=false\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_FORKJOIN_FRAMES=true\n",
      "   KMP_FORKJOIN_FRAMES_MODE=3\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_ITT_PREPARE_DELAY=0\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_MWAIT_HINTS=0\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=8M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_THREAD_LIMIT=36\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_USER_LEVEL_MWAIT=false\n",
      "   KMP_USE_YIELD=1\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=true\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %i thread %n bound to OS proc set {%A}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEBUG=disabled\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=2147483647\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED=false\n",
      "   OMP_NUM_THREADS='1'\n",
      "   OMP_PLACES: value is not defined\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=8M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_TOOL=enabled\n",
      "   OMP_TOOL_LIBRARIES: value is not defined\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='noverbose,warnings,respect,granularity=fine,compact,1,0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# recommended to set it to True when running BigDL in Jupyter notebook. \n",
    "OrcaContext.log_output = True # (this will display terminal's stdout and stderr in the Jupyter notebook).\n",
    "\n",
    "cluster_mode = \"local\"\n",
    "\n",
    "if cluster_mode == \"local\":\n",
    "    init_orca_context(cores=1, memory=\"4g\") # run in local mode\n",
    "elif cluster_mode == \"k8s\":\n",
    "    init_orca_context(cluster_mode=\"k8s\", num_nodes=2, cores=4) # run on K8s cluster\n",
    "elif cluster_mode == \"yarn\":\n",
    "    init_orca_context(\n",
    "        cluster_mode=\"yarn-client\", cores=4, num_nodes=2, memory=\"2g\",\n",
    "        driver_memory=\"10g\", driver_cores=1\n",
    "        ) # run on Hadoop YARN cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9310f",
   "metadata": {},
   "source": [
    "This is the only place where you need to specify local or distributed mode. View Orca Context for more details.\n",
    "\n",
    "**Note**: You should export HADOOP_CONF_DIR=/path/to/hadoop/conf/dir when you run on Hadoop YARN cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b80b96",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58fe6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleRankingModel(tfrs.models.Model):\n",
    "    def __init__(self, user_id_num, movie_title_num):\n",
    "        super().__init__()\n",
    "        embedding_dim = 32\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "        self.user_embedding = tf.keras.layers.Embedding(user_id_num + 1, embedding_dim)\n",
    "        self.movie_embedding = tf.keras.layers.Embedding(movie_title_num + 1, embedding_dim)\n",
    "        self.ratings = tf.keras.Sequential([\n",
    "              # Learn multiple dense layers.\n",
    "              tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "              tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "              # Make rating predictions in the final layer.\n",
    "              tf.keras.layers.Dense(1)\n",
    "          ])\n",
    "\n",
    "    def call(self, features):\n",
    "        embeddings = tf.concat([self.user_embedding(features[\"user_id\"]),\n",
    "                               self.movie_embedding(features[\"movie_title\"])], axis=1)\n",
    "        return self.ratings(embeddings)\n",
    "\n",
    "    def compute_loss(self, inputs, training: bool = False) -> tf.Tensor:\n",
    "        labels = inputs[\"user_rating\"]\n",
    "        rating_predictions = self(inputs)\n",
    "        return self.task(labels=labels, predictions=rating_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e94b4",
   "metadata": {},
   "source": [
    "### Define the dataset\n",
    "\n",
    "Use Friesian FeatureTable to get and preprocess the movielens data and split it into a training and test set.\n",
    "\n",
    "First, we will download the [ml-1m dataset](https://grouplens.org/datasets/movielens/1m/) and unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4cadd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-14 11:44:28--  https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
      "Resolving child-prc.intel.com (child-prc.intel.com)... 10.239.120.56\n",
      "Connecting to child-prc.intel.com (child-prc.intel.com)|10.239.120.56|:913... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 5917549 (5.6M) [application/zip]\n",
      "Saving to: ‘ml-1m.zip’\n",
      "\n",
      "ml-1m.zip           100%[===================>]   5.64M   102KB/s    in 32s     \n",
      "\n",
      "2022-06-14 11:45:02 (179 KB/s) - ‘ml-1m.zip’ saved [5917549/5917549]\n",
      "\n",
      "Archive:  ml-1m.zip\n",
      "   creating: ml-1m/\n",
      "  inflating: ml-1m/movies.dat        \n",
      "  inflating: ml-1m/ratings.dat       \n",
      "  inflating: ml-1m/README            \n",
      "  inflating: ml-1m/users.dat         \n"
     ]
    }
   ],
   "source": [
    "!wget https://files.grouplens.org/datasets/movielens/ml-1m.zip && unzip ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58b6b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./ml-1m/\"\n",
    "\n",
    "# UserID::MovieID::Rating::Timestamp\n",
    "# UserID::Gender::Age::Occupation::Zip-code\n",
    "# MovieID::Title::Genres\n",
    "dataset = {\n",
    "    \"ratings\": ['userid', 'movieid', 'rating', 'timestamp'],\n",
    "    \"movies\": [\"movieid\", \"title\", \"genres\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ade52",
   "metadata": {},
   "source": [
    "Then we will use Friesian FeatureTable to read the .dat files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49b1cc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tbl_dict = dict()\n",
    "for data, cols in dataset.items():\n",
    "    tbl = FeatureTable.read_csv(os.path.join(data_dir, data + \".dat\"),\n",
    "                                delimiter=\":\", header=False)\n",
    "    tmp_cols = tbl.columns[::2]\n",
    "    tbl = tbl.select(tmp_cols)\n",
    "    col_dict = {c[0]: c[1] for c in zip(tmp_cols, cols)}\n",
    "    tbl = tbl.rename(col_dict)\n",
    "    tbl_dict[data] = tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78184586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------+------+\n",
      "|userid|title                                 |rating|\n",
      "+------+--------------------------------------+------+\n",
      "|1     |One Flew Over the Cuckoo's Nest (1975)|5     |\n",
      "|1     |James and the Giant Peach (1996)      |3     |\n",
      "|1     |My Fair Lady (1964)                   |3     |\n",
      "|1     |Erin Brockovich (2000)                |4     |\n",
      "|1     |Bug's Life, A (1998)                  |5     |\n",
      "+------+--------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_tbl = tbl_dict[\"ratings\"].join(tbl_dict[\"movies\"], \"movieid\")\\\n",
    "    .dropna(columns=None).select([\"userid\", \"title\", \"rating\"])\n",
    "full_tbl = full_tbl.cast([\"rating\"], \"int\")\n",
    "full_tbl = full_tbl.cast([\"userid\"], \"string\")\n",
    "full_tbl.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c20be",
   "metadata": {},
   "source": [
    "Generate unique index value of categorical features and encode these columns with generated string indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31eba547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:==============================================>       (171 + 1) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "|rating|userid|title|\n",
      "+------+------+-----+\n",
      "|5     |4395  |1855 |\n",
      "|3     |4395  |136  |\n",
      "|3     |4395  |2973 |\n",
      "|4     |4395  |816  |\n",
      "|5     |4395  |1582 |\n",
      "+------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:=========================================>            (155 + 2) / 200]\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "str_idx = full_tbl.gen_string_idx([\"userid\", \"title\"])\n",
    "user_id_size = str_idx[0].size()\n",
    "title_size = str_idx[1].size()\n",
    "full_tbl = full_tbl.encode_string([\"userid\", \"title\"], str_idx)\n",
    "full_tbl.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65296af",
   "metadata": {},
   "source": [
    "Sample 10% data and split it into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d7da6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_tbl = full_tbl.sample(0.1, seed=42)\n",
    "train_tbl, test_tbl = part_tbl.random_split([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cbdd7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  75363 , steps:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size:  18790 , steps:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_count = train_tbl.size()\n",
    "steps = math.ceil(train_count / 8192)\n",
    "print(\"train size: \", train_count, \", steps: \", steps)\n",
    "\n",
    "test_count = test_tbl.size()\n",
    "test_steps = math.ceil(test_count / 4096)\n",
    "print(\"test size: \", test_count, \", steps: \", test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea25ac7",
   "metadata": {},
   "source": [
    "Create Orca TF Datasets from a Friesian FeatureTables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b61385fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_ds = Dataset.from_feature_table(train_tbl)\n",
    "test_ds = Dataset.from_feature_table(test_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324dc149",
   "metadata": {},
   "source": [
    "Once the Orca TF Dataset is created, we can perform some data preprocessing using the map function. Since the model use `input[\"movie_title\"], input[\"user_id\"] and input[\"user_rating\"]` in the model `call` and `compute_loss` function, we should change the key name of the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51b2b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(lambda x: {\n",
    "    \"movie_title\": x[\"title\"],\n",
    "    \"user_id\": x[\"userid\"],\n",
    "    \"user_rating\": x[\"rating\"],\n",
    "})\n",
    "test_ds = test_ds.map(lambda x: {\n",
    "    \"movie_title\": x[\"title\"],\n",
    "    \"user_id\": x[\"userid\"],\n",
    "    \"user_rating\": x[\"rating\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db4e83",
   "metadata": {},
   "source": [
    "Create an Orca Estimator using the SampleRankingModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2c339b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creator(config):\n",
    "    model = SampleRankingModel(user_id_num=user_id_size, movie_title_num=title_size)\n",
    "    model = TFRSModel(model)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adagrad(config[\"lr\"]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d0a1b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0614 11:47:19.891345796   30886 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0614 11:47:19.986957924   30886 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "2022-06-14 11:47:20,712\tINFO services.py:1340 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://10.239.158.177:8265\u001b[39m\u001b[22m\n",
      "E0614 11:47:20.717564919   30886 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0614 11:47:20.733864227   30886 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node_ip_address': '10.239.158.177', 'raylet_ip_address': '10.239.158.177', 'redis_address': '10.239.158.177:16587', 'object_store_address': '/tmp/ray/session_2022-06-14_11-47-18_502417_30886/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-06-14_11-47-18_502417_30886/sockets/raylet', 'webui_url': '10.239.158.177:8265', 'session_dir': '/tmp/ray/session_2022-06-14_11-47-18_502417_30886', 'metrics_export_port': 58752, 'node_id': 'e1cd025488daced3d9e501a5275b08b0c1dc93f16421dd3f6917d1d7'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31451)\u001b[0m /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, please use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "\u001b[2m\u001b[36m(pid=31451)\u001b[0m   warnings.warn(warning_msg)\n",
      "\u001b[2m\u001b[36m(pid=31451)\u001b[0m /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/zoo_engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, you are recommended to use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "\u001b[2m\u001b[36m(pid=31451)\u001b[0m   warnings.warn(warning_msg)\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m 2022-06-14 11:47:22.435256: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m 2022-06-14 11:47:22.435284: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/zoo_engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, you are recommended to use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m   warnings.warn(warning_msg)\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m WARNING:tensorflow:From /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/orca/learn/tf2/tf_runner.py:316: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m use distribute.MultiWorkerMirroredStrategy instead\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m 2022-06-14 11:47:23.633951: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m 2022-06-14 11:47:23.634006: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yina-intel\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m 2022-06-14 11:47:23.634022: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yina-intel\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m 2022-06-14 11:47:23.634121: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.103.1\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m 2022-06-14 11:47:23.634160: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.103.1\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m 2022-06-14 11:47:23.634174: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.103.1\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m 2022-06-14 11:47:23.634954: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m 2022-06-14 11:47:23.640822: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.239.158.177:38427}\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m 2022-06-14 11:47:23.640928: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.239.158.177:38427}\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m 2022-06-14 11:47:23.641538: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://10.239.158.177:38427\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"lr\": 0.1\n",
    "}\n",
    "\n",
    "est = Estimator.from_keras(model_creator=model_creator,\n",
    "                           verbose=True,\n",
    "                           config=config, backend=\"tf2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b43877",
   "metadata": {},
   "source": [
    "Then train the model using Orca TF Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6472d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 11:47:30.406273: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-14 11:47:30.406295: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[Stage 60:>                                                         (0 + 1) / 1]2022-06-14 11:47:31.636922: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-14 11:47:31.636987: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-14 11:47:31.637033: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-14 11:47:31.637079: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-14 11:47:31.637125: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-14 11:47:31.637173: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-14 11:47:31.637221: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-14 11:47:31.637268: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-14 11:47:31.637276: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-14 11:47:31.637534: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f2be8cf79e0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x7f2be8cf79e0>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\u001b[2m\u001b[36m(pid=31738)\u001b[0m /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, please use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "\u001b[2m\u001b[36m(pid=31738)\u001b[0m   warnings.warn(warning_msg)\n",
      "\u001b[2m\u001b[36m(pid=31738)\u001b[0m /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/zoo_engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, you are recommended to use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "\u001b[2m\u001b[36m(pid=31738)\u001b[0m   warnings.warn(warning_msg)\n",
      "2022-06-14 11:47:32,502\tINFO worker.py:843 -- Connecting to existing Ray cluster at address: 10.239.158.177:16587\n",
      "\u001b[2m\u001b[36m(LocalStore pid=31738)\u001b[0m 2022-06-14 11:47:32.858619: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "\u001b[2m\u001b[36m(LocalStore pid=31738)\u001b[0m 2022-06-14 11:47:32.858645: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m WARNING:tensorflow:From /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/orca/learn/tf2/tf_runner.py:194: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m rename to distribute_datasets_from_function\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m 2022-06-14 11:47:34.130458: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <bound method SampleRankingModel.call of <__main__.SampleRankingModel object at 0x7f555c48ce90>> and will run it as-is.\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m Cause: Unknown node type <gast.gast.Import object at 0x7f555c163310>\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10 [==>...........................] - ETA: 29s - root_mean_squared_error: 3.5497 - loss: 12.6004 - regularization_loss: 0.0000e+00 - total_loss: 12.6004\n",
      " 4/10 [===========>..................] - ETA: 0s - root_mean_squared_error: 2.7792 - loss: 7.7240 - regularization_loss: 0.0000e+00 - total_loss: 7.7240 \n",
      " 5/10 [==============>...............] - ETA: 0s - root_mean_squared_error: 2.6547 - loss: 7.0473 - regularization_loss: 0.0000e+00 - total_loss: 7.0473\n",
      " 7/10 [====================>.........] - ETA: 0s - root_mean_squared_error: 2.3238 - loss: 5.3999 - regularization_loss: 0.0000e+00 - total_loss: 5.3999\n",
      " 8/10 [=======================>......] - ETA: 0s - root_mean_squared_error: 2.2091 - loss: 4.8802 - regularization_loss: 0.0000e+00 - total_loss: 4.8802\n",
      " 9/10 [==========================>...] - ETA: 0s - root_mean_squared_error: 2.1168 - loss: 4.4809 - regularization_loss: 0.0000e+00 - total_loss: 4.4809\n",
      "10/10 [==============================] - ETA: 0s - root_mean_squared_error: 2.0450 - loss: 4.1821 - regularization_loss: 0.0000e+00 - total_loss: 4.1821\n",
      "10/10 [==============================] - 4s 67ms/step - root_mean_squared_error: 2.0450 - loss: 3.9376 - regularization_loss: 0.0000e+00 - total_loss: 3.9376\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m Epoch 2/3\n",
      " 1/10 [==>...........................] - ETA: 0s - root_mean_squared_error: 1.1958 - loss: 1.4299 - regularization_loss: 0.0000e+00 - total_loss: 1.4299\n",
      " 2/10 [=====>........................] - ETA: 0s - root_mean_squared_error: 1.1730 - loss: 1.3758 - regularization_loss: 0.0000e+00 - total_loss: 1.3758\n",
      " 4/10 [===========>..................] - ETA: 0s - root_mean_squared_error: 1.1350 - loss: 1.2882 - regularization_loss: 0.0000e+00 - total_loss: 1.2882\n",
      " 5/10 [==============>...............] - ETA: 0s - root_mean_squared_error: 1.1155 - loss: 1.2443 - regularization_loss: 0.0000e+00 - total_loss: 1.2443\n",
      " 7/10 [====================>.........] - ETA: 0s - root_mean_squared_error: 1.0864 - loss: 1.1803 - regularization_loss: 0.0000e+00 - total_loss: 1.1803\n",
      " 8/10 [=======================>......] - ETA: 0s - root_mean_squared_error: 1.0866 - loss: 1.1806 - regularization_loss: 0.0000e+00 - total_loss: 1.1806\n",
      "10/10 [==============================] - 1s 71ms/step - root_mean_squared_error: 1.1161 - loss: 1.2661 - regularization_loss: 0.0000e+00 - total_loss: 1.2661\n",
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m Epoch 3/3\n",
      " 3/10 [========>.....................] - ETA: 0s - root_mean_squared_error: 1.1951 - loss: 1.4282 - regularization_loss: 0.0000e+00 - total_loss: 1.4282\n",
      " 5/10 [==============>...............] - ETA: 0s - root_mean_squared_error: 1.1931 - loss: 1.4236 - regularization_loss: 0.0000e+00 - total_loss: 1.4236\n",
      " 9/10 [==========================>...] - ETA: 0s - root_mean_squared_error: 1.1885 - loss: 1.4125 - regularization_loss: 0.0000e+00 - total_loss: 1.4125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'train_root_mean_squared_error': 1.1877973079681396,\n",
       "  'train_loss': 1.3957912921905518,\n",
       "  'train_regularization_loss': 0.0,\n",
       "  'train_total_loss': 1.3957912921905518}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "10/10 [==============================] - 0s 34ms/step - root_mean_squared_error: 1.1878 - loss: 1.4095 - regularization_loss: 0.0000e+00 - total_loss: 1.4095\n"
     ]
    }
   ],
   "source": [
    "est.fit(train_ds, 3, batch_size=8192, steps_per_epoch=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b86fbc",
   "metadata": {},
   "source": [
    "Finally, we can evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9efcd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f2bd00dc3b0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x7f2bd00dc3b0>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\u001b[2m\u001b[36m(pid=31912)\u001b[0m /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, please use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "\u001b[2m\u001b[36m(pid=31912)\u001b[0m   warnings.warn(warning_msg)\n",
      "\u001b[2m\u001b[36m(pid=31912)\u001b[0m /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/zoo_engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, you are recommended to use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "\u001b[2m\u001b[36m(pid=31912)\u001b[0m   warnings.warn(warning_msg)\n",
      "\u001b[2m\u001b[36m(LocalStore pid=31912)\u001b[0m 2022-06-14 11:47:53.610544: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "\u001b[2m\u001b[36m(LocalStore pid=31912)\u001b[0m 2022-06-14 11:47:53.610569: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m \r",
      "1/5 [=====>........................] - ETA: 5s - root_mean_squared_error: 1.6026 - loss: 2.5685 - regularization_loss: 0.0000e+00 - total_loss: 2.5685\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "4/5 [=======================>......] - ETA: 0s - root_mean_squared_error: 1.0364 - loss: 1.0741 - regularization_loss: 0.0000e+00 - total_loss: 1.0741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'validation_root_mean_squared_error': 1.2214914560317993}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Worker pid=31451)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 24ms/step - root_mean_squared_error: 1.2215 - loss: 1.7707 - regularization_loss: 0.0000e+00 - total_loss: 1.7707\n"
     ]
    }
   ],
   "source": [
    "est.evaluate(test_ds, 4096, num_steps=test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12583d3",
   "metadata": {},
   "source": [
    "Shutdown the Estimator and stop the orca context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00cc0db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping orca context\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 11:48:01,833\tERROR worker.py:1247 -- listen_error_messages_raylet: Connection closed by server.\n",
      "2022-06-14 11:48:01,839\tERROR import_thread.py:89 -- ImportThread: Connection closed by server.\n",
      "2022-06-14 11:48:01,841\tERROR worker.py:478 -- print_logs: Connection closed by server.\n"
     ]
    }
   ],
   "source": [
    "est.shutdown()\n",
    "stop_orca_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f7f44e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
