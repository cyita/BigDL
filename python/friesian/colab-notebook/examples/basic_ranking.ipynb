{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ce9ece",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/intel-analytics/BigDL/blob/main/python/friesian/colab-notebook/examples/basic_ranking.ipynb) &nbsp;![](../../../image/GitHub-Mark-32px.png)[View source on GitHub](https://github.com/intel-analytics/BigDL/blob/main/python/friesian/colab-notebook/examples/basic_ranking.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ab60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright 2016 The BigDL Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "#\n",
    "# Copyright 2020 The TensorFlow Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "# This example is based on Tensorflow Recommenders example [basic ranking](https://www.tensorflow.org/recommenders/examples/basic_ranking).\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711ec73",
   "metadata": {},
   "source": [
    "# Basic Ranking Example\n",
    "\n",
    "In this tutorial, we're going to:\n",
    "\n",
    "1. Use Friesian FeatureTable to get and preprocess the movielens data and split it into a training and test set.\n",
    "2. Convert the preprocessed FeatureTable to an Orca TF Dataset and do some further data preprocessing.\n",
    "3. Fit and evaluate the TFRS ranking model using Orca TF Estimator and Orca TF Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f310d",
   "metadata": {},
   "source": [
    "# Environment Preparation\n",
    "\n",
    "### Install Java 8\n",
    "\n",
    "Run the cell on the **Google Colab** to install jdk 1.8.\n",
    "\n",
    "**Note**: if you run this notebook on your computer, root permission is required when running the cell to install Java 8. (You may ignore this cell if Java 8 has already been set up in your computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7914d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install jdk8\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "import os\n",
    "# Set environment variable JAVA_HOME.\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4443af51",
   "metadata": {},
   "source": [
    "### Install BigDL Friesian\n",
    "\n",
    "You can install the latest pre-release version using pip install --pre --upgrade bigdl-friesian[train]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba53a4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bigdl-friesian[train]\n",
      "  Downloading bigdl_friesian-2.1.0b20220613-py3-none-manylinux1_x86_64.whl (183 kB)\n",
      "\u001b[K     |████████████████████████████████| 183 kB 202 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bigdl-orca==2.1.0b20220613\n",
      "  Downloading bigdl_orca-2.1.0b20220613-py3-none-manylinux1_x86_64.whl (23.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.9 MB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bigdl-tf==0.14.0.dev1\n",
      "  Using cached bigdl_tf-0.14.0.dev1-py3-none-manylinux2010_x86_64.whl (71.0 MB)\n",
      "Requirement already satisfied: pyzmq in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (22.3.0)\n",
      "Requirement already satisfied: packaging in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (21.3)\n",
      "Collecting bigdl-dllib==2.1.0b20220613\n",
      "  Downloading bigdl_dllib-2.1.0b20220613-py3-none-manylinux1_x86_64.whl (51.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 51.8 MB 7.4 MB/s eta 0:00:012     |██████████████████▌             | 30.0 MB 12.2 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting bigdl-math==0.14.0.dev1\n",
      "  Using cached bigdl_math-0.14.0.dev1-py3-none-manylinux2010_x86_64.whl (35.4 MB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting conda-pack==0.3.1\n",
      "  Using cached conda_pack-0.3.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting bigdl-core==2.1.0b20220321\n",
      "  Downloading bigdl_core-2.1.0b20220321-py3-none-manylinux2010_x86_64.whl (51.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 51.0 MB 7.1 MB/s eta 0:00:012    |██████████████▏                 | 22.5 MB 1.7 MB/s eta 0:00:17\n",
      "\u001b[?25hRequirement already satisfied: pyspark==2.4.6 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-dllib==2.1.0b20220613->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-dllib==2.1.0b20220613->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.21.4)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from bigdl-dllib==2.1.0b20220613->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from conda-pack==0.3.1->bigdl-dllib==2.1.0b20220613->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (58.0.4)\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from pyspark==2.4.6->bigdl-dllib==2.1.0b20220613->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.10.7)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Collecting psutil\n",
      "  Downloading psutil-5.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
      "\u001b[K     |████████████████████████████████| 281 kB 201 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aioredis==1.3.1\n",
      "  Using cached aioredis-1.3.1-py3-none-any.whl (65 kB)\n",
      "Collecting ray[default]==1.9.2\n",
      "  Using cached ray-1.9.2-cp37-cp37m-manylinux2014_x86_64.whl (57.6 MB)\n",
      "Collecting hiredis==2.0.0\n",
      "  Using cached hiredis-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (85 kB)\n",
      "Collecting async-timeout==4.0.1\n",
      "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
      "Collecting aiohttp==3.8.1\n",
      "  Using cached aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from aiohttp==3.8.1->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (4.1.1)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from aiohttp==3.8.1->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (21.4.0)\n",
      "Collecting asynctest==0.13.0\n",
      "  Using cached asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting redis>=3.5.0\n",
      "  Downloading redis-4.3.3-py3-none-any.whl (244 kB)\n",
      "\u001b[K     |████████████████████████████████| 244 kB 202 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click>=7.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 6.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Using cached PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "Requirement already satisfied: jsonschema in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (4.4.0)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (3.19.4)\n",
      "Requirement already satisfied: grpcio>=1.28.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (1.44.0)\n",
      "Collecting msgpack<2.0.0,>=1.0.0\n",
      "  Downloading msgpack-1.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "\u001b[K     |████████████████████████████████| 299 kB 587 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opencensus\n",
      "  Downloading opencensus-0.9.0-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gpustat>=1.0.0b1\n",
      "  Using cached gpustat-1.0.0b1-py3-none-any.whl\n",
      "Collecting colorful\n",
      "  Downloading colorful-0.6.0a1-py2.py3-none-any.whl (202 kB)\n",
      "\u001b[K     |████████████████████████████████| 202 kB 592 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp-cors\n",
      "  Using cached aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting py-spy>=0.2.0\n",
      "  Downloading py_spy-0.4.0_dev2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 594 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.13.1)\n",
      "Collecting smart-open\n",
      "  Downloading smart_open-6.0.0-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 7.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 2.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from click>=7.0->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (4.11.2)\n",
      "Collecting nvidia-ml-py3>=7.352.0\n",
      "  Using cached nvidia_ml_py3-7.352.0-py3-none-any.whl\n",
      "Collecting blessed>=1.17.1\n",
      "  Using cached blessed-1.19.1-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from blessed>=1.17.1->gpustat>=1.0.0b1->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.2.5)\n",
      "Collecting redis>=3.5.0\n",
      "  Downloading redis-4.3.2-py3-none-any.whl (244 kB)\n",
      "\u001b[K     |████████████████████████████████| 244 kB 7.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading redis-4.3.1-py3-none-any.whl (241 kB)\n",
      "\u001b[K     |████████████████████████████████| 241 kB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading redis-4.3.0-py3-none-any.whl (241 kB)\n",
      "\u001b[K     |████████████████████████████████| 241 kB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading redis-4.2.2-py3-none-any.whl (226 kB)\n",
      "\u001b[K     |████████████████████████████████| 226 kB 10.6 MB/s eta 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h  Downloading redis-4.2.1-py3-none-any.whl (225 kB)\n",
      "\u001b[K     |████████████████████████████████| 225 kB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading redis-4.2.0-py3-none-any.whl (225 kB)\n",
      "\u001b[K     |████████████████████████████████| 225 kB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading redis-4.2.0rc3-py3-none-any.whl (224 kB)\n",
      "\u001b[K     |████████████████████████████████| 224 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading redis-4.2.0rc2-py3-none-any.whl (223 kB)\n",
      "\u001b[K     |████████████████████████████████| 223 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading redis-4.2.0rc1-py3-none-any.whl (220 kB)\n",
      "\u001b[K     |████████████████████████████████| 220 kB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Using cached redis-4.1.4-py3-none-any.whl (175 kB)\n",
      "Collecting deprecated>=1.2.3\n",
      "  Using cached Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 459 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from importlib-metadata->click>=7.0->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from packaging->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (3.0.7)\n",
      "Collecting idna>=2.0\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from jsonschema->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from jsonschema->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (0.18.1)\n",
      "Collecting opencensus-context>=0.1.2\n",
      "  Downloading opencensus_context-0.2.dev0-py2.py3-none-any.whl (4.5 kB)\n",
      "Collecting google-api-core<3.0.0,>=1.0.0\n",
      "  Downloading google_api_core-2.8.1-py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 269 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.2-py2.py3-none-any.whl (211 kB)\n",
      "\u001b[K     |████████████████████████████████| 211 kB 595 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.7.0-py2.py3-none-any.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 303 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from requests->ray[default]==1.9.2->bigdl-orca==2.1.0b20220613->bigdl-friesian[train]) (2021.10.8)\n",
      "Installing collected packages: pyasn1, wrapt, urllib3, rsa, pyasn1-modules, multidict, idna, frozenlist, charset-normalizer, cachetools, yarl, requests, googleapis-common-protos, google-auth, deprecated, asynctest, async-timeout, aiosignal, redis, pyyaml, psutil, opencensus-context, nvidia-ml-py3, msgpack, hiredis, google-api-core, filelock, conda-pack, click, blessed, bigdl-core, aiohttp, smart-open, ray, py-spy, opencensus, gpustat, colorful, bigdl-tf, bigdl-math, bigdl-dllib, aioredis, aiohttp-cors, setproctitle, bigdl-orca, bigdl-friesian\n",
      "Successfully installed aiohttp-3.8.1 aiohttp-cors-0.7.0 aioredis-1.3.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 bigdl-core-2.1.0b20220321 bigdl-dllib-2.1.0b20220613 bigdl-friesian-2.1.0b20220613 bigdl-math-0.14.0.dev1 bigdl-orca-2.1.0b20220613 bigdl-tf-0.14.0.dev1 blessed-1.19.1 cachetools-5.2.0 charset-normalizer-2.0.12 click-8.1.3 colorful-0.6.0a1 conda-pack-0.3.1 deprecated-1.2.13 filelock-3.7.1 frozenlist-1.3.0 google-api-core-2.8.1 google-auth-2.7.0 googleapis-common-protos-1.56.2 gpustat-1.0.0b1 hiredis-2.0.0 idna-3.3 msgpack-1.0.4 multidict-6.0.2 nvidia-ml-py3-7.352.0 opencensus-0.9.0 opencensus-context-0.2.dev0 psutil-5.9.1 py-spy-0.4.0.dev2 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyyaml-6.0 ray-1.9.2 redis-4.1.4 requests-2.28.0 rsa-4.8 setproctitle-1.2.3 smart-open-6.0.0 urllib3-1.26.9 wrapt-1.14.1 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "# Install latest pre-release version of BigDL Friesian \n",
    "# Installing BigDL Friesian from pip will automatically install pyspark and their dependencies.\n",
    "!pip install --pre --upgrade bigdl-friesian[train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b98741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 511.7 MB 16 kB/s  eta 0:00:011    |█▏                              | 19.2 MB 10.7 MB/s eta 0:00:47     |████████▊                       | 138.7 MB 9.0 MB/s eta 0:00:42     |████████████████▎               | 259.5 MB 8.2 MB/s eta 0:00:31     |███████████████████▍            | 310.0 MB 9.3 MB/s eta 0:00:22     |█████████████████████▌          | 343.2 MB 8.7 MB/s eta 0:00:20     |██████████████████████▎         | 356.0 MB 10.7 MB/s eta 0:00:15     |████████████████████████████████| 511.1 MB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-recommenders\n",
      "  Using cached tensorflow_recommenders-0.6.0-py3-none-any.whl (85 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (1.21.4)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (58.0.4)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 10.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.1.0-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (4.1.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[K     |████████████████████████████████| 438 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.7.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (1.44.0)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: packaging in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "\u001b[K     |████████████████████████████████| 224 kB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 7.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.7.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages (from packaging->tensorflow) (3.0.7)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, markdown, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow, tensorflow-recommenders\n",
      "Successfully installed absl-py-1.1.0 astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 h5py-3.7.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 tensorflow-recommenders-0.6.0 termcolor-1.1.0 werkzeug-2.1.2\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "!pip install tensorflow tensorflow-recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb54b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 21:34:09.738528: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-13 21:34:09.738555: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c4905",
   "metadata": {},
   "source": [
    "## Distributed TFRS using Orca and Friesian APIs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c0f607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, please use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "  warnings.warn(warning_msg)\n",
      "/home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/zoo_engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, you are recommended to use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from bigdl.friesian.models import TFRSModel\n",
    "from bigdl.orca import init_orca_context, stop_orca_context\n",
    "from bigdl.orca import OrcaContext\n",
    "from bigdl.friesian.feature import FeatureTable\n",
    "from bigdl.orca.learn.tf2 import Estimator\n",
    "from bigdl.orca.data.tf.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae5f1c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing orca context\n",
      "Current pyspark location is : /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py\n",
      "Start to getOrCreate SparkContext\n",
      "pyspark_submit_args is:  --driver-class-path /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/share/orca/lib/bigdl-orca-spark_2.4.6-2.1.0-SNAPSHOT-jar-with-dependencies.jar:/home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/share/dllib/lib/bigdl-dllib-spark_2.4.6-2.1.0-SNAPSHOT-jar-with-dependencies.jar:/home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/share/friesian/lib/bigdl-friesian-spark_2.4.6-2.1.0-SNAPSHOT-jar-with-dependencies.jar:/home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/share/core/lib/all-2.1.0-20220314.094552-2.jar pyspark-shell \n",
      "[main] WARN  org.apache.spark.util.Utils  - Your hostname, yina-intel resolves to a loopback address: 127.0.1.1; using 10.239.158.177 instead (on interface enp0s31f6)\n",
      "[main] WARN  org.apache.spark.util.Utils  - Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-13 21:34:25,210 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "2022-06-13 21:34:25,212 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "2022-06-13 21:34:25,212 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "2022-06-13 21:34:25,213 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "22-06-13 21:34:25 [Thread-4] INFO  Engine$:121 - Auto detect executor number and executor cores number\n",
      "22-06-13 21:34:25 [Thread-4] INFO  Engine$:123 - Executor number is 1 and executor cores number is 1\n",
      "22-06-13 21:34:25 [Thread-4] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 27\n",
      "[Thread-4] WARN  org.apache.spark.SparkContext  - Using an existing SparkContext; some configuration may not take effect.\n",
      "22-06-13 21:34:25 [Thread-4] INFO  Engine$:456 - Find existing spark context. Checking the spark conf...\n",
      "cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.Sample\n",
      "BigDLBasePickler registering: bigdl.dllib.utils.common  Sample\n",
      "cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.EvaluatedResult\n",
      "BigDLBasePickler registering: bigdl.dllib.utils.common  EvaluatedResult\n",
      "cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.JTensor\n",
      "BigDLBasePickler registering: bigdl.dllib.utils.common  JTensor\n",
      "cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.JActivity\n",
      "BigDLBasePickler registering: bigdl.dllib.utils.common  JActivity\n",
      "Successfully got a SparkContext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,compact,1,0\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_SETTINGS=1\n",
      "   OMP_NUM_THREADS=1\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=144\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_HAND_THREAD=false\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=false\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_FORKJOIN_FRAMES=true\n",
      "   KMP_FORKJOIN_FRAMES_MODE=3\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_ITT_PREPARE_DELAY=0\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_MWAIT_HINTS=0\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=8M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_THREAD_LIMIT=36\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_USER_LEVEL_MWAIT=false\n",
      "   KMP_USE_YIELD=1\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=true\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %i thread %n bound to OS proc set {%A}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEBUG=disabled\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=2147483647\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED=false\n",
      "   OMP_NUM_THREADS='1'\n",
      "   OMP_PLACES: value is not defined\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=8M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_TOOL=enabled\n",
      "   OMP_TOOL_LIBRARIES: value is not defined\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='noverbose,warnings,respect,granularity=fine,compact,1,0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# recommended to set it to True when running BigDL in Jupyter notebook. \n",
    "OrcaContext.log_output = True # (this will display terminal's stdout and stderr in the Jupyter notebook).\n",
    "\n",
    "cluster_mode = \"local\"\n",
    "\n",
    "if cluster_mode == \"local\":\n",
    "    init_orca_context(cores=1, memory=\"4g\") # run in local mode\n",
    "elif cluster_mode == \"k8s\":\n",
    "    init_orca_context(cluster_mode=\"k8s\", num_nodes=2, cores=4) # run on K8s cluster\n",
    "elif cluster_mode == \"yarn\":\n",
    "    init_orca_context(\n",
    "        cluster_mode=\"yarn-client\", cores=4, num_nodes=2, memory=\"2g\",\n",
    "        driver_memory=\"10g\", driver_cores=1\n",
    "        ) # run on Hadoop YARN cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68389302",
   "metadata": {},
   "source": [
    "This is the only place where you need to specify local or distributed mode. View Orca Context for more details.\n",
    "\n",
    "**Note**: You should export HADOOP_CONF_DIR=/path/to/hadoop/conf/dir when you run on Hadoop YARN cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c01a6ef",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2735160",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleRankingModel(tfrs.models.Model):\n",
    "    def __init__(self, user_id_num, movie_title_num):\n",
    "        super().__init__()\n",
    "        embedding_dim = 32\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "        self.user_embedding = tf.keras.layers.Embedding(user_id_num + 1, embedding_dim)\n",
    "        self.movie_embedding = tf.keras.layers.Embedding(movie_title_num + 1, embedding_dim)\n",
    "        self.ratings = tf.keras.Sequential([\n",
    "              # Learn multiple dense layers.\n",
    "              tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "              tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "              # Make rating predictions in the final layer.\n",
    "              tf.keras.layers.Dense(1)\n",
    "          ])\n",
    "\n",
    "    def call(self, features):\n",
    "        embeddings = tf.concat([self.user_embedding(features[\"user_id\"]),\n",
    "                               self.movie_embedding(features[\"movie_title\"])], axis=1)\n",
    "        return self.ratings(embeddings)\n",
    "\n",
    "    def compute_loss(self, inputs, training: bool = False) -> tf.Tensor:\n",
    "        labels = inputs[\"user_rating\"]\n",
    "        rating_predictions = self(inputs)\n",
    "        return self.task(labels=labels, predictions=rating_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ea608",
   "metadata": {},
   "source": [
    "### Define the dataset\n",
    "\n",
    "Use Friesian FeatureTable to get and preprocess the movielens data and split it into a training and test set.\n",
    "\n",
    "First, we will download the [ml-1m dataset](https://grouplens.org/datasets/movielens/1m/) and unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93203ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-13 21:42:39--  https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
      "Resolving child-prc.intel.com (child-prc.intel.com)... 10.239.120.56\n",
      "Connecting to child-prc.intel.com (child-prc.intel.com)|10.239.120.56|:913... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 5917549 (5.6M) [application/zip]\n",
      "Saving to: ‘ml-1m.zip.1’\n",
      "\n",
      "ml-1m.zip.1         100%[===================>]   5.64M  10.6KB/s    in 6m 51s  \n",
      "\n",
      "2022-06-13 21:49:31 (14.1 KB/s) - ‘ml-1m.zip.1’ saved [5917549/5917549]\n",
      "\n",
      "Archive:  ml-1m.zip\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of ml-1m.zip or\n",
      "        ml-1m.zip.zip, and cannot find ml-1m.zip.ZIP, period.\n"
     ]
    }
   ],
   "source": [
    "!wget https://files.grouplens.org/datasets/movielens/ml-1m.zip && unzip ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa2d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./ml-1m/\"\n",
    "\n",
    "# UserID::MovieID::Rating::Timestamp\n",
    "# UserID::Gender::Age::Occupation::Zip-code\n",
    "# MovieID::Title::Genres\n",
    "dataset = {\n",
    "    \"ratings\": ['userid', 'movieid', 'rating', 'timestamp'],\n",
    "    \"movies\": [\"movieid\", \"title\", \"genres\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640362a",
   "metadata": {},
   "source": [
    "Then we will use Friesian FeatureTable to read the .dat files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d11bda63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tbl_dict = dict()\n",
    "for data, cols in dataset.items():\n",
    "    tbl = FeatureTable.read_csv(os.path.join(data_dir, data + \".dat\"),\n",
    "                                delimiter=\":\", header=False)\n",
    "    tmp_cols = tbl.columns[::2]\n",
    "    tbl = tbl.select(tmp_cols)\n",
    "    col_dict = {c[0]: c[1] for c in zip(tmp_cols, cols)}\n",
    "    tbl = tbl.rename(col_dict)\n",
    "    tbl_dict[data] = tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3d5f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------+------+\n",
      "|userid|title                                 |rating|\n",
      "+------+--------------------------------------+------+\n",
      "|1     |One Flew Over the Cuckoo's Nest (1975)|5     |\n",
      "|1     |James and the Giant Peach (1996)      |3     |\n",
      "|1     |My Fair Lady (1964)                   |3     |\n",
      "|1     |Erin Brockovich (2000)                |4     |\n",
      "|1     |Bug's Life, A (1998)                  |5     |\n",
      "+------+--------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_tbl = tbl_dict[\"ratings\"].join(tbl_dict[\"movies\"], \"movieid\")\\\n",
    "    .dropna(columns=None).select([\"userid\", \"title\", \"rating\"])\n",
    "full_tbl = full_tbl.cast([\"rating\"], \"int\")\n",
    "full_tbl = full_tbl.cast([\"userid\"], \"string\")\n",
    "full_tbl.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216823f",
   "metadata": {},
   "source": [
    "Generate unique index value of categorical features and encode these columns with generated string indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49146be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:========================================>             (151 + 1) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "|rating|userid|title|\n",
      "+------+------+-----+\n",
      "|5     |4395  |1855 |\n",
      "|3     |4395  |136  |\n",
      "|3     |4395  |2973 |\n",
      "|4     |4395  |816  |\n",
      "|5     |4395  |1582 |\n",
      "+------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:====================================================> (193 + 1) / 200]\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "str_idx = full_tbl.gen_string_idx([\"userid\", \"title\"])\n",
    "user_id_size = str_idx[0].size()\n",
    "title_size = str_idx[1].size()\n",
    "full_tbl = full_tbl.encode_string([\"userid\", \"title\"], str_idx)\n",
    "full_tbl.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4073e3d",
   "metadata": {},
   "source": [
    "Sample 10% data and split it into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9866fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_tbl = full_tbl.sample(0.1, seed=42)\n",
    "train_tbl, test_tbl = part_tbl.random_split([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "645027a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  75420 , steps:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size:  18733 , steps:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_count = train_tbl.size()\n",
    "steps = math.ceil(train_count / 8192)\n",
    "print(\"train size: \", train_count, \", steps: \", steps)\n",
    "\n",
    "test_count = test_tbl.size()\n",
    "test_steps = math.ceil(test_count / 4096)\n",
    "print(\"test size: \", test_count, \", steps: \", test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5997883",
   "metadata": {},
   "source": [
    "Create Orca TF Datasets from a Friesian FeatureTables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bd43bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_ds = Dataset.from_feature_table(train_tbl)\n",
    "test_ds = Dataset.from_feature_table(test_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa4e69",
   "metadata": {},
   "source": [
    "Once the Orca TF Dataset is created, we can perform some data preprocessing using the map function. Since the model use `input[\"movie_title\"], input[\"user_id\"] and input[\"user_rating\"]` in the model `call` and `compute_loss` function, we should change the key name of the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d46de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(lambda x: {\n",
    "    \"movie_title\": x[\"title\"],\n",
    "    \"user_id\": x[\"userid\"],\n",
    "    \"user_rating\": x[\"rating\"],\n",
    "})\n",
    "test_ds = test_ds.map(lambda x: {\n",
    "    \"movie_title\": x[\"title\"],\n",
    "    \"user_id\": x[\"userid\"],\n",
    "    \"user_rating\": x[\"rating\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed756d",
   "metadata": {},
   "source": [
    "Create an Orca Estimator using the SampleRankingModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdb10939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creator(config):\n",
    "    model = SampleRankingModel(user_id_num=user_id_size, movie_title_num=title_size)\n",
    "    model = TFRSModel(model)\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  optimizer=tf.keras.optimizers.Adagrad(config[\"lr\"]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e339e391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0613 21:54:36.217899060   24654 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0613 21:54:36.315804290   24654 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "2022-06-13 21:54:37,060\tINFO services.py:1340 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://10.239.158.177:8265\u001b[39m\u001b[22m\n",
      "E0613 21:54:37.103059549   24654 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0613 21:54:37.138308369   24654 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node_ip_address': '10.239.158.177', 'raylet_ip_address': '10.239.158.177', 'redis_address': '10.239.158.177:62454', 'object_store_address': '/tmp/ray/session_2022-06-13_21-54-34_822277_24654/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-06-13_21-54-34_822277_24654/sockets/raylet', 'webui_url': '10.239.158.177:8265', 'session_dir': '/tmp/ray/session_2022-06-13_21-54-34_822277_24654', 'metrics_export_port': 60426, 'node_id': '6c8ec3a3fef337d5614b65e196de5f9298f48f6def7666854d6cf674'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25637)\u001b[0m /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, please use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "\u001b[2m\u001b[36m(pid=25637)\u001b[0m   warnings.warn(warning_msg)\n",
      "\u001b[2m\u001b[36m(pid=25637)\u001b[0m /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/zoo_engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, you are recommended to use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "\u001b[2m\u001b[36m(pid=25637)\u001b[0m   warnings.warn(warning_msg)\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m 2022-06-13 21:54:38.842553: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m 2022-06-13 21:54:38.842578: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/zoo_engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, you are recommended to use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m   warnings.warn(warning_msg)\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m WARNING:tensorflow:From /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/orca/learn/tf2/tf_runner.py:316: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m use distribute.MultiWorkerMirroredStrategy instead\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m 2022-06-13 21:54:40.062259: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m 2022-06-13 21:54:40.062304: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yina-intel\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m 2022-06-13 21:54:40.062317: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yina-intel\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m 2022-06-13 21:54:40.062404: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.103.1\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m 2022-06-13 21:54:40.062437: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.103.1\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m 2022-06-13 21:54:40.062447: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.103.1\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m 2022-06-13 21:54:40.063214: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m 2022-06-13 21:54:40.069934: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.239.158.177:41275}\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m 2022-06-13 21:54:40.070042: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.239.158.177:41275}\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m 2022-06-13 21:54:40.070757: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://10.239.158.177:41275\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"lr\": 0.1\n",
    "}\n",
    "\n",
    "est = Estimator.from_keras(model_creator=model_creator,\n",
    "                           verbose=True,\n",
    "                           config=config, backend=\"tf2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b1ecf",
   "metadata": {},
   "source": [
    "Then train the model using Orca TF Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "377a2f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 21:54:44.507848: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-13 21:54:44.507872: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[Stage 60:>                                                         (0 + 1) / 1]2022-06-13 21:54:45.739910: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-13 21:54:45.740053: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-13 21:54:45.740157: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-13 21:54:45.740259: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-13 21:54:45.740361: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-13 21:54:45.740463: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-13 21:54:45.740564: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-13 21:54:45.740667: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "2022-06-13 21:54:45.740682: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-13 21:54:45.741067: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f9f066c79e0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x7f9f066c79e0>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\u001b[2m\u001b[36m(pid=25928)\u001b[0m /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, please use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "\u001b[2m\u001b[36m(pid=25928)\u001b[0m   warnings.warn(warning_msg)\n",
      "\u001b[2m\u001b[36m(pid=25928)\u001b[0m /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/zoo_engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, you are recommended to use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "\u001b[2m\u001b[36m(pid=25928)\u001b[0m   warnings.warn(warning_msg)\n",
      "2022-06-13 21:54:46,609\tINFO worker.py:843 -- Connecting to existing Ray cluster at address: 10.239.158.177:62454\n",
      "\u001b[2m\u001b[36m(LocalStore pid=25928)\u001b[0m 2022-06-13 21:54:46.988546: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "\u001b[2m\u001b[36m(LocalStore pid=25928)\u001b[0m 2022-06-13 21:54:46.988573: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m WARNING:tensorflow:From /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/orca/learn/tf2/tf_runner.py:194: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m rename to distribute_datasets_from_function\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m 2022-06-13 21:54:48.327823: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <bound method SampleRankingModel.call of <__main__.SampleRankingModel object at 0x7ef5e67c3cd0>> and will run it as-is.\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m Cause: Unknown node type <gast.gast.Import object at 0x7ef5e64972d0>\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10 [==>...........................] - ETA: 29s - root_mean_squared_error: 3.6143 - loss: 13.0630 - regularization_loss: 0.0000e+00 - total_loss: 13.0630\n",
      " 3/10 [========>.....................] - ETA: 0s - root_mean_squared_error: 2.8407 - loss: 8.0697 - regularization_loss: 0.0000e+00 - total_loss: 8.0697  \n",
      " 5/10 [==============>...............] - ETA: 0s - root_mean_squared_error: 2.7096 - loss: 7.3420 - regularization_loss: 0.0000e+00 - total_loss: 7.3420\n",
      " 6/10 [=================>............] - ETA: 0s - root_mean_squared_error: 2.6026 - loss: 6.7733 - regularization_loss: 0.0000e+00 - total_loss: 6.7733\n",
      " 8/10 [=======================>......] - ETA: 0s - root_mean_squared_error: 2.3295 - loss: 5.4265 - regularization_loss: 0.0000e+00 - total_loss: 5.4265\n",
      " 9/10 [==========================>...] - ETA: 0s - root_mean_squared_error: 2.2273 - loss: 4.9609 - regularization_loss: 0.0000e+00 - total_loss: 4.9609\n",
      "10/10 [==============================] - 4s 64ms/step - root_mean_squared_error: 2.1493 - loss: 4.3399 - regularization_loss: 0.0000e+00 - total_loss: 4.3399\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m Epoch 2/3\n",
      " 1/10 [==>...........................] - ETA: 0s - root_mean_squared_error: 1.2083 - loss: 1.4599 - regularization_loss: 0.0000e+00 - total_loss: 1.4599\n",
      " 2/10 [=====>........................] - ETA: 0s - root_mean_squared_error: 1.1862 - loss: 1.4070 - regularization_loss: 0.0000e+00 - total_loss: 1.4070\n",
      " 4/10 [===========>..................] - ETA: 0s - root_mean_squared_error: 1.1348 - loss: 1.2878 - regularization_loss: 0.0000e+00 - total_loss: 1.2878\n",
      " 5/10 [==============>...............] - ETA: 0s - root_mean_squared_error: 1.1161 - loss: 1.2457 - regularization_loss: 0.0000e+00 - total_loss: 1.2457\n",
      " 6/10 [=================>............] - ETA: 0s - root_mean_squared_error: 1.0966 - loss: 1.2025 - regularization_loss: 0.0000e+00 - total_loss: 1.2025\n",
      " 8/10 [=======================>......] - ETA: 0s - root_mean_squared_error: 1.0865 - loss: 1.1805 - regularization_loss: 0.0000e+00 - total_loss: 1.1805\n",
      " 9/10 [==========================>...] - ETA: 0s - root_mean_squared_error: 1.1032 - loss: 1.2170 - regularization_loss: 0.0000e+00 - total_loss: 1.2170\n",
      "10/10 [==============================] - 1s 70ms/step - root_mean_squared_error: 1.1132 - loss: 1.2575 - regularization_loss: 0.0000e+00 - total_loss: 1.2575\n",
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m Epoch 3/3\n",
      " 1/10 [==>...........................] - ETA: 0s - root_mean_squared_error: 1.1912 - loss: 1.4190 - regularization_loss: 0.0000e+00 - total_loss: 1.4190\n",
      " 5/10 [==============>...............] - ETA: 0s - root_mean_squared_error: 1.1856 - loss: 1.4056 - regularization_loss: 0.0000e+00 - total_loss: 1.4056\n",
      " 7/10 [====================>.........] - ETA: 0s - root_mean_squared_error: 1.1880 - loss: 1.4112 - regularization_loss: 0.0000e+00 - total_loss: 1.4112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'train_root_mean_squared_error': 1.1844418048858643,\n",
       "  'train_loss': 1.3926191329956055,\n",
       "  'train_regularization_loss': 0.0,\n",
       "  'train_total_loss': 1.3926191329956055}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 9/10 [==========================>...] - ETA: 0s - root_mean_squared_error: 1.1849 - loss: 1.4040 - regularization_loss: 0.0000e+00 - total_loss: 1.4040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "10/10 [==============================] - ETA: 0s - root_mean_squared_error: 1.1844 - loss: 1.4029 - regularization_loss: 0.0000e+00 - total_loss: 1.4029\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "10/10 [==============================] - 0s 35ms/step - root_mean_squared_error: 1.1844 - loss: 1.4020 - regularization_loss: 0.0000e+00 - total_loss: 1.4020\n"
     ]
    }
   ],
   "source": [
    "est.fit(train_ds, 3, batch_size=8192, steps_per_epoch=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b790a8",
   "metadata": {},
   "source": [
    "Finally, we can evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15ad84cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f9ef005f290> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x7f9ef005f290>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\u001b[2m\u001b[36m(pid=26101)\u001b[0m /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, please use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "\u001b[2m\u001b[36m(pid=26101)\u001b[0m   warnings.warn(warning_msg)\n",
      "\u001b[2m\u001b[36m(pid=26101)\u001b[0m /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/bigdl/dllib/utils/zoo_engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /opt/spark-2.4.3-bin-hadoop2.7, and pyspark is found in: /home/yina/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, you are recommended to use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "\u001b[2m\u001b[36m(pid=26101)\u001b[0m   warnings.warn(warning_msg)\n",
      "\u001b[2m\u001b[36m(LocalStore pid=26101)\u001b[0m 2022-06-13 21:55:05.499700: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/d1/Dockers/grpcwnd/lib\n",
      "\u001b[2m\u001b[36m(LocalStore pid=26101)\u001b[0m 2022-06-13 21:55:05.499728: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Worker pid=25637)\u001b[0m \r",
      "1/5 [=====>........................] - ETA: 4s - root_mean_squared_error: 1.6395 - loss: 2.6881 - regularization_loss: 0.0000e+00 - total_loss: 2.6881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'validation_root_mean_squared_error': 1.2274852991104126}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 25ms/step - root_mean_squared_error: 1.2275 - loss: 1.7887 - regularization_loss: 0.0000e+00 - total_loss: 1.7887\n"
     ]
    }
   ],
   "source": [
    "est.evaluate(test_ds, 4096, num_steps=test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6601ef",
   "metadata": {},
   "source": [
    "Shutdown the Estimator and stop the orca context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e7fb394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping orca context\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 21:55:14,972\tERROR worker.py:1247 -- listen_error_messages_raylet: Connection closed by server.\n",
      "2022-06-13 21:55:14,975\tERROR import_thread.py:89 -- ImportThread: Connection closed by server.\n",
      "2022-06-13 21:55:14,978\tERROR worker.py:478 -- print_logs: Connection closed by server.\n"
     ]
    }
   ],
   "source": [
    "est.shutdown()\n",
    "stop_orca_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc9d034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
